#Modeling

# Load ML libraries
library(mlr)           # ML toolkit
library(randomForest)  # Random Forest
library(adabag)        # Boosting
library(e1071)         # SVM
library("dummies")

basetable<-read.csv("basetable4.csv")
basetable<-basetable[,-c(1)]


# Subsetting data
set.seed(1000)
sample_rows = sample.split(basetable$HasDetections, SplitRatio=0.01)
data_2 = basetable[ sample_rows,]

#Checking that there are no unitary variables (with only 1 value)
which(sapply(data_2, function(x) length(unique(x))<2))

# Drop problematic variable: UacLuaenable
data_3<-data_2[,-c(4,23,24)]

# Checking target distribution
table(data_3$HasDetections)

# Creating dummies for factors for those models that don't support factors:
data_3$MachineIdentifier<-as.numeric(data_3$MachineIdentifier)
data_3$GeoNameIdentifier<-as.numeric(data_3$GeoNameIdentifier)
data_3$Census_FirmwareManufacturerIdentifier<-as.numeric(data_3$Census_FirmwareManufacturerIdentifier)
data_3$IeVerIdentifier<-as.numeric(data_3$IeVerIdentifier)
data_3$Census_OSInstallLanguageIdentifier<-as.numeric(data_3$Census_OSInstallLanguageIdentifier)
data_4<-data_3
data_4$HasDetections<-as.numeric(data_4$HasDetections)
data_4<-dummy.data.frame(data_4, names = NULL, omit.constants=TRUE, dummy.classes = getOption("dummy.classes"))
data_4$HasDetections<-as.factor(data_4$HasDetections)
# Split the data into train/test again
set.seed(1)
data_split <- sample(1:nrow(data_3), round((nrow(data_3)*0.7)))
train <- data_3[data_split, ]  # 70%
test <- data_3[-data_split, ]  # 30%
# Split the data into train/test again
train_1 <- data_4[data_split, ]  # 70%
test_1 <- data_4[-data_split, ]  # 30%

#################       Modelling           ###############################

# Define the ML classification task
train_task <- makeClassifTask(id ='Train', data=train, target='HasDetections')
test_task <- makeClassifTask(id='Test', data=test, target='HasDetections')
train_task_1 <- makeClassifTask(id ='Train', data=train_1, target='HasDetections')
test_task_1 <- makeClassifTask(id='Test', data=test_1, target='HasDetections')

#normalize the variables
train_task <- normalizeFeatures(train_task,method = "standardize")
test_task <- normalizeFeatures(test_task,method = "standardize")
train_task_1 <- normalizeFeatures(train_task_1,method = "standardize")
test_task_1 <- normalizeFeatures(test_task_1,method = "standardize")


############## Logistic Regresion #######################
##########################################################
learner_lg <- makeLearner('classif.logreg', predict.type="prob")
model_lg <- mlr::train(learner_lg, train_task)
pred_test_lg <- predict(model_lg, task=test_task)
accuracy_lg<-performance(pred_test_lg, measures=acc)
f1_lg<-performance(pred_test_lg, measures=f1)
recall_lg<-performance(pred_test_lg, measures=tpr)
auc_lg<-performance(pred_test_lg, measures=auc)
logloss_lg<-performance(pred_test_lg, measures=logloss)
plot_lg<-plotLearnerPrediction(learner_lg, task=test_task, measures =auc)
d = generateThreshVsPerfData(pred_test_lg, measures = list(fpr, fnr))
plotThreshVsPerf(d)
plotROCCurves(d)

############### Decision Tree ##########################
##########################################################
learner_dt <- makeLearner('classif.rpart',predict.type="prob")
model_dt <- mlr::train(learner_dt, train_task)
pred_test_dt <- predict(model_dt, task=test_task)
accuracy_dt<-performance(pred_test_dt, measures=acc)
f1_dt<-performance(pred_test_dt, measures=f1)
recall_dt<-performance(pred_test_dt, measures=tpr)
auc_dt<-performance(pred_test_dt, measures=auc)
logloss_dt<-performance(pred_test_dt, measures=logloss)
plot_dt<-plotLearnerPrediction(learner_dt, task=test_task, measures =auc)


############### Random Forest ##########################
##########################################################
learner_rf <- makeLearner('classif.randomForest',predict.type="prob")
model_rf <- mlr::train(learner_rf, train_task)
pred_test_rf <- predict(model_rf, task=test_task)
accuracy_rf<-performance(pred_test_rf, measures=acc)
f1_rf<-performance(pred_test_rf, measures=f1)
recall_rf<-performance(pred_test_rf, measures=tpr)
auc_rf<-performance(pred_test_rf, measures=auc)
logloss_rf<-performance(pred_test_rf, measures=logloss)
plot_rf<-plotLearnerPrediction(learner_rf, task=test_task, measures =acc)


############### Linear Dismininant Analysis ################
##########################################################
learner_lda <- makeLearner('classif.lda',predict.type="prob")
model_lda <- mlr::train(learner_lda, filterFeatures(train_task_1, method='variance', threshold=0.1))
pred_test_lda <- predict(model_lda, task=test_task_1)
accuracy_lda<-performance(pred_test_lda, measures=acc)
f1_lda<-performance(pred_test_lda, measures=f1)
recall_lda<-performance(pred_test_lda, measures=tpr)
auc_lda<-performance(pred_test_lda, measures=auc)
logloss_lda<-performance(pred_test_lda, measures=logloss)
plot_lda<-plotLearnerPrediction(learner_lda, task=test_task_1, measures =auc)


############### Support Vector Machines ################
##########################################################
learner_svm <- makeLearner('classif.svm',predict.type="prob")
model_svm <- mlr::train(learner_svm, filterFeatures(train_task_1, method='variance', threshold=0.1))
pred_test_svm <- predict(model_svm, task=test_task_1)
accuracy_svm<-performance(pred_test_svm, measures=acc)
f1_svm<-performance(pred_test_svm, measures=f1)
recall_svm<-performance(pred_test_svm, measures=tpr)
auc_svm<-performance(pred_test_svm, measures=auc)
logloss_svm<-performance(pred_test_svm, measures=logloss)
plot_svm<-plotLearnerPrediction(learner_svm, task=test_task_1, measures =auc)


############### Boosting  #############################
##########################################################
learner_boo <- makeLearner('classif.boosting',predict.type="prob")
model_boo <- mlr::train(learner_boo, train_task)
pred_test_boo <- predict(model_boo, task=test_task)
accuracy_boo<-performance(pred_test_boo, measures=acc)
f1_boo<-performance(pred_test_boo, measures=f1)
recall_boo<-performance(pred_test_boo, measures=tpr)
auc_boo<-performance(pred_test_boo, measures=auc)
logloss_boo<-performance(pred_test_boo, measures=logloss)
plot_boo<-plotLearnerPrediction(learner_boo, task=test_task, measures =auc)
d = generateThreshVsPerfData(pred_test_boo, measures = list(fpr, fnr))
plotThreshVsPerf(d)
plotROCCurves(d)


############### Neural Networks  ########################
##########################################################
learner_nn <- makeLearner('classif.nnet',predict.type="prob")
model_nn <- mlr::train(learner_nn, train_task)
pred_test_nn <- predict(model_nn, task=test_task)
accuracy_nn<-performance(pred_test_nn, measures=acc)
f1_nn<-performance(pred_test_nn, measures=f1)
recall_nn<-performance(pred_test_nn, measures=tpr)
auc_nn<-performance(pred_test_nn, measures=auc)
logloss_nn<-performance(pred_test_nn, measures=logloss)
plot_nn<-plotLearnerPrediction(learner_nn, task=test_task, measures =acc)


# Comparing between models
# Accuracy
Accuracy <- rbind(accuracy_lg,accuracy_dt,accuracy_rf,accuracy_lda,accuracy_svm,accuracy_boo, accuracy_nn)
rownames(Accuracy) <- c("Logistic Regression: ", "Decision Tree: ", "Random Forest: ", "Linear Discrimiant Analysis: ", "Support Vector Machines: ","Boosting", "Neural Networks: ")
Accuracy

# F1
F1 <- rbind(f1_lg,f1_dt,f1_rf,f1_lda,f1_svm,f1_boo, f1_nn)
rownames(F1) <- c("Logistic Regression: ", "Decision Tree: ", "Random Forest: ", "Linear Discrimiant Analysis: ", "Support Vector Machines: ","Boosting", "Neural Networks: ")
F1

# Recall
Recall <- rbind(recall_lg,recall_dt,recall_rf,recall_lda,recall_svm,recall_boo, recall_nn)
rownames(Recall) <- c("Logistic Regression: ", "Decision Tree: ", "Random Forest: ", "Linear Discrimiant Analysis: ", "Support Vector Machines: ","Boosting", "Neural Networks: ")
Recall

# AUC
auc <- rbind(auc_lg,auc_dt,auc_rf,auc_lda,auc_svm,auc_boo, auc_nn)
rownames(auc) <- c("Logistic Regression: ", "Decision Tree: ", "Random Forest: ", "Linear Discrimiant Analysis: ", "Support Vector Machines: ","Boosting", "Neural Networks: ")
auc

# Logloss
logloss <- rbind(logloss_lg,logloss_dt,logloss_rf,logloss_lda,logloss_svm,logloss_boo, logloss_nn)
rownames(logloss) <- c("Logistic Regression: ", "Decision Tree: ", "Random Forest: ", "Linear Discrimiant Analysis: ", "Support Vector Machines: ","Boosting", "Neural Networks: ")
logloss

All<-cbind(Accuracy,F1,Recall,auc, logloss)
# Saving results
write.csv(All, "eval_metrics.csv")

